#!/usr/bin/env python3
# coding: utf8

"""
Loads and handles training and validation data collections.
"""

__author__ = 'Andreas Kaufmann, Jona Braun, Frederike LÃ¼beck, Akanksha Baranwal'
__email__ = "ankaufmann@student.ethz.ch, jonbraun@student.ethz.ch, fluebeck@student.ethz.ch, abaranwal@student.ethz.ch"

import os

import numpy as np

from source.configuration import Configuration
from source.data import transformation
from source.data.datacollection import DataCollection, DataCollectionExperiment
from source.logcreator.logcreator import Logcreator

from source.data.dataset import RoadSegmentationDataset, SimpleToTensorDataset, RoadSegmentationDatasetInference


class DataPreparator(object):

    @staticmethod
    def load_all(engine, path='', is_train=False):
        """
        Loads all data in one dataset.

        :param engine: The engine.
        :param path: Path to the data folder.
        :param is_train: True = The set is used for training: load dataset with training transformations.
        :return: dataset containing all images and masks
        """
        if not path:
            path = Configuration.get_path('data_collection.folder', False)

        collection_folders = Configuration.get('data_collection.collection_names')

        # detect special experiment dataset
        if DataCollectionExperiment.experiments_dataset_folder in collection_folders:
            data_collection_exp = DataCollectionExperiment(path, collection_folders)

            # get image and respective mask paths
            train_set_imgs, train_set_masks, val_set_imgs, val_set_masks = \
                data_collection_exp.get_experiment_image_paths()

            # combine lists
            images = train_set_imgs + val_set_imgs
            masks = train_set_masks + val_set_masks
        else:
            data_collection = DataCollection(path, collection_folders)

            # get image and respective mask paths
            images_orig, masks_orig = data_collection.get_original_images()
            images_trans, masks_trans = data_collection.get_transformed_images()

            # combine lists
            images = images_orig + images_trans
            masks = masks_orig + masks_trans

        return DataPreparator.get_dataset(engine, images, masks, is_train, name="All")

    @staticmethod
    def load_train_val(engine, path=''):
        """
        Loads the training and validation data.

        :param engine: The engine.
        :param path: Path to the data folder.
        :return: train dataset, validation dataset
        """
        if not path:
            path = Configuration.get_path('data_collection.folder', False)

        collection_folders = Configuration.get('data_collection.collection_names')

        # detect special experiment dataset
        if DataCollectionExperiment.experiments_dataset_folder in collection_folders:
            train_ds, val_ds = DataPreparator.experiment_run_datasets(engine, path, collection_folders)
            return train_ds, val_ds

        data_collection = DataCollection(path, collection_folders)

        # Get image and respective mask paths
        images_orig, masks_orig = data_collection.get_original_images()
        images_trans, masks_trans = data_collection.get_transformed_images()

        # Get train / validation split
        train_set_images, train_set_masks, val_set_images, val_set_masks = \
            DataPreparator.get_train_validation_split(images_orig, masks_orig, images_trans, masks_trans)

        # Get datasets
        train_ds, val_ds = DataPreparator.get_datasets(engine, train_set_images, train_set_masks,
                                                       val_set_images, val_set_masks)

        return train_ds, val_ds

    @staticmethod
    def load_test(engine, path='', crop_size=(608, 608)):
        """
        Loads the test data.

        :param engine: The engine.
        :param path: Path to the data folder.
        :param crop_size:  The size to which the images should be cropped.
        :return: test dataset
        """
        if not path:
            path = Configuration.get_path('data_collection.folder', False)

        test_images = [os.path.join(path, filename) for filename in os.listdir(path)]

        transform = transformation.get_transformations(engine=engine, is_train=False)

        return RoadSegmentationDatasetInference(engine, test_images, transform=transform, crop_size=crop_size)

    @staticmethod
    def get_train_validation_split(images_orig, masks_orig, images_trans, masks_trans):
        """
        Splits the image list into a training and validation set according to the configuration.

        :param images_orig: image path list
        :param masks_orig: mask path list
        :param images_trans: image transformed path list
        :param masks_trans: mask transformed path list
        :return: train image path list, train mask path list, validation image path list, validation mask path list
        """
        val_ratio = Configuration.get('data_collection.validation_ratio', default=0.2)

        # Create mask for validation set
        train_set_images = []
        train_set_masks = []
        val_set_images = []
        val_set_masks = []
        size_val_set = int(len(images_orig) * val_ratio)
        mask = np.full(len(images_orig), False)
        mask[:size_val_set] = True
        np.random.seed(0)
        np.random.shuffle(mask)

        # Define validation set
        for idx, add_to_val_set in enumerate(mask):
            if add_to_val_set:
                val_set_images.append(images_orig[idx])
                val_set_masks.append(masks_orig[idx])
            else:
                train_set_images.append(images_orig[idx])
                train_set_masks.append(masks_orig[idx])

        # Add transformations to training set
        for idx, image_path in enumerate(images_trans):
            split_path = os.path.normpath(image_path).split(os.sep)
            img_name = split_path[-1]
            collection = split_path[-4]
            red_list = list(filter(lambda j: img_name in j, list(filter(lambda k: collection in k, val_set_images))))
            if len(red_list) == 0:
                train_set_images.append(image_path)
                train_set_masks.append(masks_trans[idx])
            else:
                if Configuration.get('data_collection.include_val_transforms'):
                    val_set_images.append(image_path)
                    val_set_masks.append(masks_trans[idx])

        return train_set_images, train_set_masks, val_set_images, val_set_masks

    @staticmethod
    def get_datasets(engine, train_set_images, train_set_masks, val_set_images, val_set_masks):
        """
        Gets the training and validation datasets.

        :param engine: The engine.
        :param train_set_images: train image list
        :param train_set_masks: train mask list
        :param val_set_images: validation image list
        :param val_set_masks: validation mask list
        :return: training dataset, validation dataset
        """
        train_ds = DataPreparator.get_dataset(engine, train_set_images, train_set_masks,
                                              is_train=True,
                                              name="Training",
                                              compute_stats=True)

        val_ds = DataPreparator.get_dataset(engine, val_set_images, val_set_masks,
                                            is_train=False,
                                            name="Validation")

        return train_ds, val_ds

    @staticmethod
    def get_dataset(engine, images, masks, is_train, name="training", compute_stats=False):
        """
        Gets a single dataset based on the provided image and mask list.

        :param engine: The engine.
        :param images: image path list
        :param masks: mask path list
        :param is_train: True = training transforms are applied, False = validation transforms are applied
        :param name: The name for logging purpose.
        :param compute_stats: True = compute mean and std of the dataset.
        :return: dataset
        """
        Logcreator.h1(name, "set contains " + str(len(images)) + " images")
        if len(images) == 0:
            Logcreator.warn("No ", name, "files assigned.")

        # Create dataset
        foreground_threshold = Configuration.get("training.general.foreground_threshold")
        cropped_image_size = tuple(Configuration.get("training.general.cropped_image_size"))
        use_submission_masks = Configuration.get("training.general.use_submission_masks")
        min_road_percentage = Configuration.get("data_collection.min_road_percentage", optional=True, default=0)
        include_overlapping_patches = Configuration.get("data_collection.include_overlapping_patches",
                                                        optional=True, default=True)

        # get image transforms
        image_transforms = transformation.get_transformations(engine=engine, is_train=is_train)

        ds = RoadSegmentationDataset(engine, images, masks, foreground_threshold,
                                     transform=image_transforms,
                                     crop_size=cropped_image_size,
                                     use_submission_masks=use_submission_masks,
                                     min_road_percentage=min_road_percentage,
                                     include_overlapping_patches=include_overlapping_patches)

        if compute_stats:
            mean_after, std_after = transformation.get_mean_std(ds)
            Logcreator.info(f"Mean and std after transformations: mean {mean_after}, std {std_after}")

        return ds

    @staticmethod
    def experiment_run_datasets(engine, path, collection_folders):
        """
        Loads the special experiments dataset.

        :param engine: The engine.
        :param path: Path to the data folder.
        :param collection_folders: The data collection folder list.
        :return: training dataset, validation dataset
        """
        data_collection_exp = DataCollectionExperiment(path, collection_folders)

        train_set_imgs, train_set_masks, val_set_imgs, val_set_masks = \
            data_collection_exp.get_experiment_image_paths()

        train_ds, val_ds = DataPreparator.get_datasets(engine, train_set_imgs, train_set_masks,
                                                       val_set_imgs, val_set_masks)
        return train_ds, val_ds
